{
 "cells": [
  {
   "cell_type": "raw",
   "id": "ee21639c",
   "metadata": {},
   "source": [
    "Una de las cosas mas importantes en el analisis de datos es la Limpieza de estos\n",
    "para eso tenemos varias herramientas pero todas tiene casos de uso\n",
    "recordemos siempre que no todos los proyectos requerieren los mismos por eso\n",
    "dejare 6 tipos de limpieza de datos mas utilizados y que estare usando en mis\n",
    "proyectos\n",
    "veamos el paso a paso de cada uno"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b3bfbb4d",
   "metadata": {},
   "source": [
    "1) Identificar y eliminar datos duplicados "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c6050f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#las filas duplicadas serán eliminadas, los valores de la columna 'id' no se alteran; \n",
    "#simplemente se eliminan las filas duplicada\n",
    "df_sin_duplicados = df.drop_duplicates(subset='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb5396f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sin_duplicados = df.drop.duplicates(subset=[‘columna1’,’columna2’ ])\n",
    "#En esta línea, construimos otro dataFrame eliminando los valores que se encuentren repetidos\n",
    "#considerando las columnas ‘columna1’ y ‘columna2’."
   ]
  },
  {
   "cell_type": "raw",
   "id": "32c95c39",
   "metadata": {},
   "source": [
    "2) tratar valores atipicos (outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fd7827",
   "metadata": {},
   "outputs": [],
   "source": [
    "#por ejemplo tenemos la columna edad y las edades son entre 1 y 99 años, pero algunos pasan o no llegan\n",
    "#usaremos su percentil entre 1 y 99\n",
    "percentil_99 = df['edad'].quantile(0.99)\n",
    "percentil_1 = df['edad'].quantile(0.09)\n",
    "df_sin_outliers = df[(df['edad']>= percentil_1) & (df['edad'] <= percentil_99)]\n",
    "\n",
    "#un ejemplo mas complejo es el siguiente\n",
    "#Un criterio habitual para seleccionar solo los datos no atípicos es utilizar como referencia el rango\n",
    "#intercuartil, y los cuartiles 1 y 3. Así, nos quedamos solo con los valores que se encuentran en el\n",
    "#intervalo [Q1 - 1,5*IQR, Q3 + 1,5*IQR]\n",
    "data = df['columna']\n",
    "Q1 = np.percentile(data, 25)\n",
    "Q3 = np.percentile(data, 75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "df=df.iloc[np.where((data>= lower_bound) * (data <= upper_bound))]\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "31cda42d",
   "metadata": {},
   "source": [
    "3) imputar valores faltantes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a615ecb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#por ejemplo si tenemos una columna llamada \"INGRESOS\" y en este faltan datos tenemos 2 opciones\n",
    "#mostrar las filas que estan con errores o datos incorrectos y ver a quien pertenecen\n",
    "#comparando otra columna o simplemente\n",
    "#en este caso usaremos primero la MEDIA y el valor que este nos de, lo usaremos para sustituir los faltantes\n",
    "\n",
    "#PRIMERO CALCULAMOS LA MEDIA y con fillna sustituiomos con el 'valor' de ingresos_promedio\n",
    "ingresos_promedio = df['ingresos'].mean()\n",
    "df['ingresos'] = df['ingresos'].fillna(ingresos_promedio)\n",
    "\n",
    "df['columna'].fillna(df['columna'].mean(), inplace=True)\n",
    "#En esta línea, reemplazamos los valores faltantes de ‘columna’ por el valor promedio de los datos\n",
    "#de ‘columna’"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b0bcbe2e",
   "metadata": {},
   "source": [
    "4) estandarizar formatos de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2605aa80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#en este caso estandarizamos las fechas\n",
    "df['fecha'] = pd.to_datetime(df['fecha'])\n",
    "#si deseas que todas las fechas estén en el formato 'YYYY-MM-DD'\n",
    "df['fecha'] = df['fecha'].dt.strftime('%Y-%m-%d')\n",
    "#podemos ordenar las fecha\n",
    "df = df.sort_values(by='fecha')\n",
    "#tambien podemos ordenarlas por la mas antigua\n",
    "fecha_mas_antigua = df['fecha'].min()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7a2b714f",
   "metadata": {},
   "source": [
    "5) corregir errores en los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29195011",
   "metadata": {},
   "outputs": [],
   "source": [
    "correcciones = {\n",
    "    'New York': 'New York',\n",
    "    'Los Angles': 'Los Angeles',\n",
    "    'Chicgao': 'Chicago',\n",
    "    #Agrega mas correcciones según sea necesario\n",
    "}\n",
    "\n",
    "#aplica las correcciones utilizando el metodo .replace()\n",
    "df['ciudad'] = df['ciudad'].replace(correcciones)\n",
    "\n",
    "#usaremos un ejemplo mas, como es el tipeo del sexo\n",
    "#supongamos que con la columna 'SEXO' tenemos la opcion de 'mujer' y no se escribio asi, fue 'mjr'\n",
    "df['sexo']= df['sexo'].replace('mjr', 'palabra correcta')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a49ab1d3",
   "metadata": {},
   "source": [
    "6) normalizacion de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca4c76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "\n",
    "#supongamos que tienes un DataFrame df con las características que deseas normalizar\n",
    "#por ejemplo, vamos a asumir que quieres normalizar las columnas 'feature1' y 'feature2'\n",
    "\n",
    "#creamos un DataFrame de ejemplo\n",
    "data = {'feature1': [10, 20, 30, 40],\n",
    "        'feature2': [5, 15, 25, 35]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "#creamos un objeto MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "#ajustamos el scaler a los datos y luego transformamos los datos\n",
    "df_normalized = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
    "\n",
    "#df_normalized ahora contiene las características normalizadas\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "05237a69",
   "metadata": {},
   "source": [
    "*** OTROS TIPOS DE LIMPIEZA DE DATOS MAS AVANZADOS ***\n",
    "\n",
    "1)Imputación avanzada de valores perdidos: En lugar de simplemente rellenar valores faltantes con la media o la mediana, puedes utilizar técnicas más avanzadas como la imputación mediante modelos predictivos, como K-Nearest Neighbors (KNN), Árboles de Decisión o Métodos de Regresión.\n",
    "\n",
    "2)Detección y tratamiento de valores atípicos: Identificar y manejar valores atípicos puede ser crucial para el análisis de datos preciso. Puedes utilizar métodos estadísticos como el rango intercuartílico (IQR) o técnicas más avanzadas como Isolation Forest o DBSCAN para detectar y tratar los valores atípicos.\n",
    "\n",
    "3)Normalización de texto: Si estás trabajando con datos de texto, puede ser útil normalizar el texto mediante técnicas como la conversión a minúsculas, eliminación de puntuación, eliminación de stopwords y lematización o derivación (stemming) de palabras.\n",
    "\n",
    "4)Detección y corrección de errores tipográficos: Puedes utilizar técnicas como la distancia de edición (Levenshtein) o el algoritmo Soundex para detectar y corregir errores tipográficos en datos de texto.\n",
    "\n",
    "5)Consolidación de categorías: Si tienes variables categóricas con muchas categorías diferentes, puedes consolidar categorías similares para reducir la cardinalidad de la variable y simplificar el análisis.\n",
    "\n",
    "6)Tratamiento de datos desbalanceados: Si estás trabajando con conjuntos de datos desbalanceados, donde una clase es mucho más frecuente que otras, puedes utilizar técnicas como submuestreo, sobremuestreo o métodos de generación de datos sintéticos para equilibrar las clases.\n",
    "\n",
    "7)Validación y limpieza de datos duplicados: Además de eliminar filas duplicadas, puedes realizar una validación más avanzada para identificar registros duplicados basados en múltiples características y decidir cómo manejarlos, ya sea fusionándolos, eliminándolos o investigando más a fondo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4736d57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "1)\n",
    "import pandas as pd\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# Crear un DataFrame de ejemplo con valores perdidos\n",
    "data = {'edad': [10, 20, None, 40, 50]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Crear un objeto KNNImputer\n",
    "imputer = KNNImputer(n_neighbors=2)\n",
    "\n",
    "# Aplicar la imputación de KNN a los datos\n",
    "df['edad_imputada'] = imputer.fit_transform(df[['edad']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bf563c",
   "metadata": {},
   "outputs": [],
   "source": [
    "2)\n",
    "# Calcular el rango intercuartílico (IQR)\n",
    "Q1 = df['ingresos'].quantile(0.25)\n",
    "Q3 = df['ingresos'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Identificar los límites del rango para los valores atípicos\n",
    "lower_limit = Q1 - 1.5 * IQR\n",
    "upper_limit = Q3 + 1.5 * IQR\n",
    "\n",
    "# Filtrar los valores atípicos\n",
    "df_sin_atipicos = df[(df['ingresos'] > lower_limit) & (df['ingresos'] < upper_limit)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b989088",
   "metadata": {},
   "outputs": [],
   "source": [
    "3) \n",
    "import string\n",
    "\n",
    "def limpiar_texto(texto):\n",
    "    texto = texto.lower()  # Convertir a minúsculas\n",
    "    texto = texto.translate(str.maketrans('', '', string.punctuation))  # Eliminar puntuación\n",
    "    return texto\n",
    "\n",
    "df['comentario_normalizado'] = df['comentario'].apply(limpiar_texto)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08bc680",
   "metadata": {},
   "outputs": [],
   "source": [
    "4) \n",
    "from fuzzywuzzy import process\n",
    "\n",
    "# Suponiendo que tienes una lista de ciudades correctas\n",
    "ciudades_correctas = ['New York', 'Los Angeles', 'Chicago']\n",
    "\n",
    "# Función para encontrar la mejor coincidencia de cada ciudad en la lista de ciudades correctas\n",
    "def corregir_ciudad(ciudad):\n",
    "    mejor_coincidencia = process.extractOne(ciudad, ciudades_correctas)\n",
    "    return mejor_coincidencia[0]\n",
    "\n",
    "df['ciudad_corregida'] = df['ciudad'].apply(corregir_ciudad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb93e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "5)\n",
    "categorias_a_consolidar = {'California': 'Oeste', 'New York': 'Este', 'Texas': 'Sur'}\n",
    "\n",
    "def consolidar_estado(estado):\n",
    "    if estado in categorias_a_consolidar:\n",
    "        return categorias_a_consolidar[estado]\n",
    "    else:\n",
    "        return estado\n",
    "\n",
    "df['estado_consolidado'] = df['estado'].apply(consolidar_estado)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff6291e",
   "metadata": {},
   "outputs": [],
   "source": [
    "6)\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "# Crear un objeto RandomOverSampler\n",
    "sobremuestreador = RandomOverSampler(random_state=42)\n",
    "\n",
    "# Aplicar el sobremuestreo a los datos\n",
    "X_resampled, y_resampled = sobremuestreador.fit_resample(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab232234",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
